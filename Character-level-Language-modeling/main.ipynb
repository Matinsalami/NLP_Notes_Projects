{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87aa4633",
   "metadata": {},
   "source": [
    "In this project the book \"The mysterious Island\" by Jules Verne is use to build a text generation model using RRN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ab770",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a177924",
   "metadata": {},
   "source": [
    "First we download the dataset or \"the book\" from gutenberg project website as a text file. Then we can create a variable that holds all the unique words in the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c296969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_length:  1130779\n",
      "Unique characters:  86\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"pg1268.txt\", \"r\", encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('total_length: ' , len(text))\n",
    "print('Unique characters: ', len(char_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e1f47",
   "metadata": {},
   "source": [
    "Now we should turn the string data into numeric values. For this, we make a function to take each character and turn it into an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5117ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array([char2int[ch] for ch in text],\n",
    "                        dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e178914",
   "metadata": {},
   "source": [
    "what we want to do is really important, we have a classification task. In which we should be able to predict which letter should come after the other. For example if we have \"Deep Learnin\", the model should be able to predict that the next letter is 'g'. So we have a classification task with an output size of `charset` which in this case is 86 characters. This is litteraly, generating next character based on a multiclass classification task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644838d",
   "metadata": {},
   "source": [
    "## Preparing text sequences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6c55ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "391f5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the data into 41 characters chunk\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i:i+chunk_size]\n",
    "               for i in range(len(text_encoded)-chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10f1e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010d22e",
   "metadata": {},
   "source": [
    "We create a Pytorch dataset so we can turn it into a DataLoader for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eb0ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text_chunk = self.text_chunks[index]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "    \n",
    "\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a0218",
   "metadata": {},
   "source": [
    "Let's look at some example from this transformed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3479280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOU'\n",
      "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS'\n",
      "Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS'\n",
      "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS '\n",
      "Input (x): 'E MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS '\n",
      "Target (y): ' MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS I'\n",
      "Input (x): ' MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS I'\n",
      "Target (y): 'MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS IS'\n",
      "Input (x): 'MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS IS'\n",
      "Target (y): 'YSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS ISL'\n",
      "Input (x): 'YSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS ISL'\n",
      "Target (y): 'STERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS ISLA'\n",
      "Input (x): 'STERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS ISLA'\n",
      "Target (y): 'TERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS ISLAN'\n",
      "Input (x): 'TERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS ISLAN'\n",
      "Target (y): 'ERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS ISLAND'\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(('Input (x):'), repr(''.join(char_array[seq])))\n",
    "    print(('Target (y):'), repr(''.join(char_array[target])))\n",
    "\n",
    "    if i== 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9805697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 64\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe2204",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a48d781",
   "metadata": {},
   "source": [
    "## Building a character-level RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3358a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        \n",
    "        out = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))  # (batch, seq_len, hidden_size)\n",
    "        out = self.fc(out)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        out = out.reshape(-1, out.size(2))  # (batch*seq_len, vocab_size)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size, device=\"gpu\"):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size, device=device)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size, device=device)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0386a20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(86, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=86, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "model = RNN(vocab_size=vocab_size, embed_dim=embed_dim, rnn_hidden_size=rnn_hidden_size).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5641f4e",
   "metadata": {},
   "source": [
    "### Loss function with optimzer and a schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0162cd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scheduler = optim.lr_scheduler.OneCycleLR(\\n     optimizer,\\n     max_lr=1e-3,                       \\n     steps_per_epoch=len(seq_dl), \\n    epochs=num_epochs\"\\n)'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "num_epochs = 10000\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\"\"\"scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "     optimizer,\n",
    "     max_lr=1e-3,                       \n",
    "     steps_per_epoch=len(seq_dl), \n",
    "    epochs=num_epochs\"\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e7f34e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10000 [00:00<31:40,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 102/10000 [00:17<28:34,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, loss: 1.3092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 202/10000 [00:34<28:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, loss: 1.3550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 302/10000 [00:52<30:03,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, loss: 1.3463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 402/10000 [01:10<29:01,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400, loss: 1.3144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 502/10000 [01:28<26:45,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, loss: 1.3549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 602/10000 [01:46<28:36,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600, loss: 1.3399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 702/10000 [02:03<28:13,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700, loss: 1.3635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 802/10000 [02:22<31:28,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800, loss: 1.4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 902/10000 [02:39<27:21,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900, loss: 1.3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1002/10000 [02:57<26:44,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, loss: 1.3579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1055/10000 [03:06<26:22,  5.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     pred, hidden, cell = model(seq_batch[:, c].unsqueeze(\u001b[32m1\u001b[39m), hidden, cell)\n\u001b[32m     16\u001b[39m     loss += loss_fn(pred, target_batch[:, c])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    \n",
    "    # move data to GPU\n",
    "    seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    # initialize hidden and cell on GPU\n",
    "    hidden, cell = model.init_hidden(batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c].unsqueeze(1), hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss: {loss.item()/seq_length:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801776bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
