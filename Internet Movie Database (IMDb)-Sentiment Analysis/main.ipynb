{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f3714e",
   "metadata": {},
   "source": [
    "# Preparing the IMDB movei revieww data for text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa260d2",
   "metadata": {},
   "source": [
    "## 1. Obtaining the movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d21d9",
   "metadata": {},
   "source": [
    "I downloaded the dataset from https://ai.stanford.edu/~amaas/data/sentiment/. Then I extracted the data with WinRar. This can also be done with python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ae987",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the movie dataset into a more convenient mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f3872",
   "metadata": {},
   "source": [
    "A. I use pyprind to track the downloading and time for completion of a specific task.\n",
    "\n",
    "B. Then I collected all the text files and store them in a Pandas dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9780d45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  I went and saw this movie last night after bei...          1\n",
      "1  Actor turned director Bill Paxton follows up h...          1\n",
      "2  As a recreational golfer with some knowledge o...          1\n",
      "3  I saw this film in a sneak preview, and it is ...          1\n",
      "4  Bill Paxton has taken the true story of the 19...          1\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "basepath = r'C:\\Users\\matin\\OneDrive\\Desktop\\Code\\NLP\\NLP_Notes_Projects\\Internet Movie Database (IMDb)-Sentiment Analysis\\aclImdb_v1\\aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "\n",
    "docs = []\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            docs.append([txt, labels[l]])\n",
    "            pbar.update()\n",
    "\n",
    "df = pd.DataFrame(docs, columns=['review', 'sentiment'])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a565c",
   "metadata": {},
   "source": [
    "Save the data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6431374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"imdb_reviews.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98af37b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb44d3",
   "metadata": {},
   "source": [
    "# Introducing the bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0ce4c",
   "metadata": {},
   "source": [
    "## 1. Transforming wors into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e4952e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining',\n",
    "'The weather is sweet',\n",
    "'The sun is shining, the weather is sweet and one and one is two'])\n",
    "\n",
    "#bag is a vector based on the indices of the vocabulary, it contains number of times a specific word is present in a sentence. \n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "# We now, have a vocabulary containing all the words which are repeated in all the sentences. \n",
    "vocabulary = count.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f884bf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [2, 3, 2, 1, 1, 1, 2, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa10beff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 6,\n",
       " 'sun': 4,\n",
       " 'is': 1,\n",
       " 'shining': 3,\n",
       " 'weather': 8,\n",
       " 'sweet': 5,\n",
       " 'and': 0,\n",
       " 'one': 2,\n",
       " 'two': 7}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe771eb",
   "metadata": {},
   "source": [
    "## 2. Assessing word relevancy via term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e22954",
   "metadata": {},
   "source": [
    "The overall odea of this method is to show the importance of each word in a semantic way in each sentence. For example, if we have 3 sentences and in all of them we have the word 'good', then this word has no special meaning when we try to distinguish sentences from each oher. Otherwise, if we have a word 'girl' which is present in only one sentence, we can calculate its tf-idf. Again, we use sklearn for converting our `CountVectorizor` into a `tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3662061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedce41",
   "metadata": {},
   "source": [
    "## 3. Cleaning text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463039d",
   "metadata": {},
   "source": [
    "Now we have to clear out texts because we have many punctuations and other non-letter characters that should be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aadbce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i went and saw this movie last night after being coaxed to by a few friends of mine i ll admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)  # use r''\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub(r'[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "preprocessor(df.loc[0, 'review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b53cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bb46f",
   "metadata": {},
   "source": [
    "## 4. Processing documents into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45559bd",
   "metadata": {},
   "source": [
    "Now we need a tokenizer to tokenize documents into individual words.\n",
    "\n",
    "Another useful technique is called **Word stemming**, which is the process of transforming a word into its root form. We then apply the function in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f105e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(text) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed3791",
   "metadata": {},
   "source": [
    "# Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf37be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
