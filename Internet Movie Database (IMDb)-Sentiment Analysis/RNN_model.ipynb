{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d30936f",
   "metadata": {},
   "source": [
    "# Preparing the movie review data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ff201",
   "metadata": {},
   "source": [
    "First I download the dataset from huggingFace community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a214df93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43050eac",
   "metadata": {},
   "source": [
    "Then I turn the trainset into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac0ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(dataset[\"train\"], [20000, 5000])\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1c92a",
   "metadata": {},
   "source": [
    "Now I use a costum tokenizer to choose unique words. In the counter I have a dictionary which connects each word with its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb62aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size: 69023\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "for line in train_dataset:\n",
    "    text = line[\"text\"]\n",
    "    tokens = tokenizer(text)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('Vocab_size:' , len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121be90",
   "metadata": {},
   "source": [
    "Now I encode each unique token into integers. index 1 is reserved for those tokens which are present in test data and are not present in training data. So we assign all of them to 1. Index 0 is explained later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a7b0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 69025\n",
      "[('<pad>', 0), ('<unk>', 1), ('the', 2), ('and', 3), ('a', 4), ('of', 5), ('to', 6), ('is', 7), ('it', 8), ('in', 9), ('i', 10), ('this', 11), ('that', 12), ('s', 13), ('was', 14), ('as', 15), ('for', 16), ('with', 17), ('movie', 18), ('but', 19)]\n"
     ]
    }
   ],
   "source": [
    "# 1. Sort tokens by frequency\n",
    "sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 2. Assign IDs\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for idx, (token, _) in enumerate(sorted_tokens, start=2):\n",
    "    vocab[token] = idx\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(list(vocab.items())[:20])  # first 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772d3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab.get(token, vocab[\"<unk>\"]) for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b1d4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65436bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for example in batch:\n",
    "        _label = example[\"label\"]\n",
    "        _text = example[\"text\"]\n",
    "\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "    label_list = torch.tensor(label_list)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return padded_text_list, label_list, lengths\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataLoader = DataLoader(train_dataset,\n",
    "                              batch_size=4,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98194e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(train_dataLoader))\n",
    "\n",
    "# Label_batch contains labels of each sentence whether it is positive(1) or negative(0) obviously for 4 sentences\n",
    "# text_batch contains contains the sentence turned into a tensor of numbers, each number representing a word in the dictionary\n",
    "# length is the length of the review before padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4971eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d4f55",
   "metadata": {},
   "source": [
    "## Embedding layers for sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177ef9c",
   "metadata": {},
   "source": [
    "Now that we created DataLoaders, we are ready for modelling. however there is still one issue. We should turn the tokens which are integers now into a vector of unique numbers to avoid the curse of dimensionality. For this case we use an embedding matrix to first reduce the size of the each input vector, and then turn these pure integers into a vector of real numbers which are between [-1,1]. For example if we have 100,000 tokens we can represent them with a vector of dimension=100.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80037249",
   "metadata": {},
   "source": [
    "# Building an RNN model for Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5c718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        # num_embeddings is the vocabulary size(Or the number of token + 2)||embedding dimension is the dimension of output\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embed_size,padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_size,rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size,fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden , cell) = self.rnn(out)\n",
    "        out = hidden[-1, : , :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be31083d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device_agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4ad1fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69025, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "model = RNN(vocab_size=VOCAB_SIZE, embed_size=embed_dim,rnn_hidden_size=rnn_hidden_size, fc_hidden_size=fc_hidden_size).to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc81047",
   "metadata": {},
   "source": [
    "## Train and Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51f5bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim,\n",
    "          loss_fn: nn.Module,\n",
    "          model: nn.Module,\n",
    "          device = device):\n",
    "    \n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0 \n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:,0]\n",
    "        loss = loss_fn(pred, label_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch\n",
    "        ).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db6bdbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader: torch.utils.data.DataLoader,\n",
    "             loss_fn: nn.Module,\n",
    "             model: nn.Module,\n",
    "             device = device):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0 , 0 \n",
    "    with torch.inference_mode():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:,0]\n",
    "            loss = loss_fn(pred, label_batch.float())\n",
    "            total_acc += (\n",
    "                (pred >= 0.5).float()  == label_batch\n",
    "            ).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5b829cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function for the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2\n",
    ")\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2984933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:14<11:07, 74.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 accuracy: 0.7586 val_accuracy: 0.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:30<10:02, 75.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 accuracy: 0.8199 val_accuracy: 0.8154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:45<08:48, 75.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 accuracy: 0.8442 val_accuracy: 0.8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:02<07:34, 75.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 accuracy: 0.8769 val_accuracy: 0.8172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [06:15<06:13, 74.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 accuracy: 0.9053 val_accuracy: 0.8336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [07:33<05:03, 75.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 accuracy: 0.9064 val_accuracy: 0.8286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [08:51<03:49, 76.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 accuracy: 0.9347 val_accuracy: 0.8448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [10:08<02:33, 76.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 accuracy: 0.9472 val_accuracy: 0.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [11:25<01:16, 76.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 accuracy: 0.9544 val_accuracy: 0.8444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:41<00:00, 76.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 accuracy: 0.9653 val_accuracy: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    acc_train, loss_train = train(train_dl,\n",
    "                                  optimizer=optimizer,\n",
    "                                  loss_fn=loss_fn,\n",
    "                                  model=model, device=device\n",
    "                                  )\n",
    "    acc_valid, loss_valid = evaluate(valid_dl,\n",
    "                                     loss_fn=loss_fn,\n",
    "                                     model=model,\n",
    "                                     device=device)\n",
    "    \n",
    "    scheduler.step(loss_valid)\n",
    "\n",
    "    print(f'Epoch: {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab8c15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1abb239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.8515\n"
     ]
    }
   ],
   "source": [
    "acc_test, _ = evaluate(test_dl,\n",
    "                       loss_fn=loss_fn,\n",
    "                       model=model)\n",
    "print(f'test_accuracy: {acc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa98cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
